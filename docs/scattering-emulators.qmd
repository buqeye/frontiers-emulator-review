---
output: html_notebook
format:
  html:
    code-fold: show
    code-tools: true
jupyter: python3
---

# Scattering Emulators {#sec-scattering-emulators}

::: {.hidden}
{{< include macros.qmd >}}
:::

{{< include _code_preamble.qmd >}}


In this section we describe the various MOR emulators one could construct for quantum scattering systems.
Throughout, we note the relationship between the variational principles from which emulators have been constructed in recent works [@Furnstahl:2020abp; @Drischler:2021qoy; @Zhang:2021jmi; @Melendez:2021lyq; @KVP_vs_NVP:2022].
We also describe how each of the results from VPs could instead be derived from Galerkin projections.


## Kohn Emulators {#sec-kohn}


### Theory {#sec-kohn-theory}

We would like to solve the SchrÃ¶dinger equation for scattering systems (at center-of-mass $E > 0$) across a range of parameters $\param$:
$$
\begin{aligned}
    H(\param) \ket{\psi} = E \ket{\psi}.
\end{aligned}
$$ {#eq-schrodinger}
We assume that $E = q^2 / 2\mu$ is fixed throughout and we suppress the $E$ dependence of $\ket{\psi_E} = \ket{\psi}$; the process can be repeated separately across a set of energies to create a suite of emulators, if desired.
The type of emulator that could be built from @eq-schrodinger is not unique, but rather depends on choices made by the modeler, such as the boundary conditions imposed on $\psi$.
It is known that if one writes
$$
\begin{aligned}
\braket{r | \psi} = \psi(r) = \frac{u(r)}{r} Y_{\ell}^m(\Omega_r)
\end{aligned}
$$
then one of the boundary conditions is given by
$$
\begin{aligned}
u(0) = 0.
\end{aligned}
$$
<!-- However, the second boundary condition needed to completely specify the solution to @eq-schrodinger is yet to be chosen. -->
Here we impose the final boundary condition by asserting that $\ket{\psi}$ satisfies the following Lippmann-Schwinger equations for wave functions
$$
\begin{aligned}
 \ket{\psi} = \ket{\phi} + G_0 K \ket{\phi},
\end{aligned}
$$
where $K$ is the real reactance matrix, $\ket{\phi}$ is the free-space wave function, and $G_0 = (H_0 - E)^{-1}$ in principle value.

We begin building our MOR emulator by first writing @eq-schrodinger in integral form.
Here we choose the general Kohn variational principle (KVP), which states that
$$
\begin{aligned}
    \mathcal{K}[\psi] = K - \frac{2\mu}{q} \braket{\psi | [H - E] | \psi}
\end{aligned}
$$ {#eq-kvp}
where $K$ is the on-shell reactance matrix, $\mu$ is the reduced mass, and $E = q^2/2\mu$ [@Furnstahl:2020abp; @Drischler:2021qoy].
The functional yields $\mathcal{K}[\psi] = K$ when $\psi$ is an exact wave function, and is a stationary approximation otherwise: $\mathcal{K}[\psi + \delta\psi] = K + \mathcal{O}(\delta K^2)$.
Rather than finding a wave function $\ket{\psi}$ that satisfies @eq-schrodinger, our task now has now changed to finding a wave function that makes @eq-kvp stationary.

The key to creating an efficient emulator from @eq-kvp follows from a trial wave function ansatz
$$
\begin{aligned}
    \ket{\widetilde\psi} \equiv \sum_{i=1}^{N_b} \beta_i \ket{\psi_i}
\end{aligned}
$$ {#eq-trial_ansatz_kohn}
where $\{\ket{\psi_i}\}$ is the exact solution to @eq-schrodinger for a choice of $\param_i$.

Inserting @eq-trial_ansatz_kohn into @eq-kvp yields
$$
\mathcal{K}[\psitrial] = \beta_i K_i - \frac{1}{2} \beta_i \dU_{ij}\beta_j
$$ {#eq-kvp_reduced}
where we have defined
$$
\begin{aligned}
    \dU_{ij} & \equiv 2\mu \braket{\psi_i | [H - E] | \psi_j} \notag\\
    & ~+ 2\mu \braket{\psi_j | [H - E] | \psi_i} \notag\\
    & = 2\mu \braket{\psi_i | [V(\param) - V_j] | \psi_j} \notag\\
    & ~+ 2\mu \braket{\psi_j | [V(\param) - V_i] | \psi_i},
\end{aligned}
$$ {#eq-delta_u_tilde}
In the final line we have added and subtracted $V_i \equiv V(\param_i)$ and $V_j \equiv V(\param_j)$ to use @eq-schrodinger.
A consequence of this formulation is that only short-range physics would be involved since the long-ranged potentials, such as Coulomb, would drop out of @eq-delta_u_tilde if the fine structure constant is fixed.
Emulating $\psi$ (via @eq-trial_ansatz_kohn), and hence $K$ (via @eq-kvp_reduced), has now reduced to determining the values of $\weights$ that make @eq-kvp_reduced stationary under the constraint that $\sum_i \weights_i = 1$ such that the wave functions remain normalized.
Such a solution can be found using a Lagrange multiplier $\lambda$, and is given by
$$
\begin{aligned} %\label{eq:coeff_solution}
    \begin{pmatrix}
        \dU & \vec{1}\,{} \\
        \vec{1} \, {}^\intercal & 0\,{}
    \end{pmatrix}
    \begin{pmatrix}
        \kvpweights \, {} \\
        \lagmult \, {}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \vec{K} \\
        1
    \end{pmatrix}.
\end{aligned}
$$ {#eq-coeff_solution}
The fact that @eq-coeff_solution is a linear system means that if $N_b$, the number of basis functions, is much smaller than the size of $\psi$, then this can be a highly computationally efficient emulator for scattering systems.

As the basis size $N_b$ increase, the matrix $\dU$ becomes increasingly ill-conditioned, which is due to the collinearity of the basis states. A way to mitigate this issue is through the use of regularization techniques. Furnstahl et al. [@Furnstahl:2020abp] proposed adding a small regularization parameter known as a nugget to the diagonal.

<!--
Everything we have done thus far is representation-independent.
The only difference between emulating in coordinate- and momentum-space is how we calculate the basis functions $\psi_i$ used to construct @eq-trial_ansatz_kohn, and thus the manner in which $\dU$ is evaluated in @eq-delta_u_tilde.
If one had already obtained $\{\psi_i\}$ in coordinate space, then $\dU$ could be straightforwardly evaluated.

In momentum space, we instead initially solve for the $K$ matrix and must relate $K$ to $\psi$ before using @eq-delta_u_tilde.
There are a couple relations one could use to accomplish this goal: the definition of the scattering wave function using the $K$ matrix
$$
\begin{aligned}
    \ket{\psi} = \ket{\phi} + G_0 K \ket{\phi},
\end{aligned}
$$
where $\phi$ is the free-space wave function, or the definition of $K$ itself
$$
\begin{aligned}
    K\ket{\phi} \equiv V\ket{\psi}.
\end{aligned}
$$
The first relation yields
$$
\begin{aligned}
    \dU_{ij}^{\ell'\ell}(\param) & = \braket{\phi_{\ell'} | \Delta V_{j}(\param) | \phi_{\ell}}
    + \braket{\phi_{\ell'} | \Delta V_{j}(\param) G_0 K_j | \phi_{\ell}} \notag\\
    & + \braket{\phi_{\ell'} | K_i G_0 \Delta V_{j}(\param) | \phi_{\ell}} \notag\\
    & + \braket{\phi_{\ell'} | K_i G_0 \Delta V_{j}(\param) G_0 K_j | \phi_{\ell}} + (i \leftrightarrow j),
\end{aligned}
$$
where sums over coupled states are implied by the operator products, and we have defined
$$
\begin{aligned}
    \Delta V_{i}(\param) \equiv V(\param) - V_i
\end{aligned}
$$
for convenience.
The latter relation yields
$$
\begin{aligned}
    \dU_{ij}^{\ell'\ell}(\param) & = \braket{\phi_{\ell'} | K_i V_i^{-1} \Delta V_{j}(\param) V_j^{-1} K_j | \phi_{\ell}} + (i \leftrightarrow j).
\end{aligned}
$$
-->

The efficient evaluation of $\dU$ across a range of $\param$ values is critical to the applicability of the emulator.
This is achieved due to the affine dependence of $V$ on the parameters $\param$:
$$
\begin{aligned}
    V(\param) = V^0 + \sum_{i=1}^{\nbasis} \theta_i V^1_i,
\end{aligned}
$$
which implies
$$
\begin{aligned}
    \dU(\param) = \dU^0 + \sum_{i=1}^{\nbasis} \theta_i \Delta \widetilde {U}^1_i,
\end{aligned}
$$ {#eq-dU0_dU1}
where the superscripts $0$ and $1$ refer to parameter-independent and -dependent terms, respectively.
This allows each term in @eq-delta_u_tilde to be evaluated up front and stored.
The value of $\dU(\param)$ at any new parameter value is then easily reconstructed from each term.

By considering @eq-dU0_dU1, the emulation process can then be separated into two parts: the offline and online stage.
In the offline stage the emulator is trained and @eq-delta_u_tilde is calculated.
The online stage is then where we obtain the emulator predictions, $\ie$, evaluate @eq-dU0_dU1 for a given set of $\param$ and solve for $\kvpweights$ using @eq-coeff_solution.


### Removing the Lagrange multiplier {#sec-kohn-no-lagrange}


The Lagrange multiplier can be removed from the emulator equations by solving for it in terms of other known quantities.
By starting with
$$
\lagmult \vec{1} = \vec{K} - \dU \coeffs
$$
one can write
$$
\lagmult = \frac{1}{\nbasis} \left[\vec{1}\cdot\vec{K} - \vec{1}^T\dU \coeffs\right].
$$
Now substituting this in the reduced KVP
$$
\begin{aligned}
\mathcal{K} & = K_i\coeff_i - \frac{1}{2} \coeff_i \dU_{ij} \coeff_j - \lagmult \left[\vec{1}\cdot\coeffs - 1\right]
\end{aligned}
$$
and enforcing stationarity yields
$$
M\coeffs = \vec{K} - \frac{1}{\nbasis} \left[\dU \vec{1} + \vec{K}\cdot\vec{1}\right]
$$ {#eq-kohn-solution-no-lagrange}
where
$$
M_{ij} = \dU_{ij} - \frac{1}{\nbasis} \sum_{k} \left[\dU_{ik} + \dU_{kj}\right].
$$
This formulation requires solving a matrix of one fewer dimension than @eq-coeff_solution and @eq-kohn-solution-no-lagrange tends to be a better conditioned system to solve as well.


### Generalized Kohn Variational Principle {#sec-kohn-general}
The KVP function, @eq-kvp, can be extended to include arbitrary boundary conditions. We define $\genkvp$ as a general functional for a general scattering matrix L,
$$
\begin{aligned}
    \mathcal{L}[\psi] = L - \frac{2\mu}{q \, \mathrm{det} \umatrix} \braket{\psi | [H - E] | \psi},
\end{aligned}
$$ {#eq-kvp-general}
where $\umatrix$ are non-singular matrices used to parametrize the asymptotic boundary conditions [@PhysRevA.40.6879; @Drischler:2021qoy; @KVP_vs_NVP:2022].

The general KVP functional may not always provide a stationary approximation. 
The consequence is spurious singularites known as Kohn (Schwartz) anomalies [@PhysRev.124.1468; @nesbet1980variational]. 
These anomalies appear at random energies and depend on the parameters $\param$ used to train the emulator in the offline stage and the evaluation set used in the online stage. 
Using @eq-kvp-general one can evaluate different KVPs for different boundary conditions and use the predictions to detect any singularities.

Drischler et al. [@Drischler:2021qoy] proposed using multiple emulators evaluated at different boundary conditions as a way to mitigate these anomalies.
The process involves calculating an estimated (mixed) $S$ matrix from a weighted sum of averages using KVP pairs that pass a consistency check.
This new estimated $S$ matrix will now be the emulator prediction.
For more information see [@Drischler:2021qoy; @KVP_vs_NVP:2022].

### Coupled Systems {#sec-kohn-coupled}

In this section we have not yet considered partial wave projections because the KVP is more general.
However, calculations are sometimes more conveniently performed after a partial-wave expansion has occurred.
In these cases, it is useful to understand how the KVP decomposes as well.

Let $s'$ and $s$ denote the incoming and outgoing channels, respectively.
(If the channel is uncoupled, we could replace $ss' \to \ell$ throughout the following.)
The KVP then generalizes simply to
$$
\mathcal{K}^{ss'} = \beta_i K_i^{ss'} - \frac{1}{2} \beta_i \dU_{ij}^{ss'}\beta_j
$$ {#eq-kvp_reduced_coupled}
where
$$
\begin{aligned}
    \dU_{ij}^{ss'} & \equiv 2\mu \braket{\psi_i^{st} | [H - E]^{tt'} | \psi_j^{t's'}} \notag\\
    & ~+ 2\mu \braket{\psi_j^{st} | [H - E]^{tt'} | \psi_i^{t's'}} \notag\\
    & = 2\mu \braket{\psi_i^{st} | [V(\param) - V_j]^{tt'} | \psi_j^{t's'}} \notag\\
    & ~+ 2\mu \braket{\psi_j^{st} | [V(\param) - V_i]^{tt'} | \psi_i^{t's'}}.
\end{aligned}
$$ {#eq-delta_u_tilde_coupled}
Solving for $\coeffs$ now proceeds as in @eq-coeff_solution but for a specific choice of $ss'$ channels.
Note that the coefficients $\coeffs$ are to be determined *independently* for each $ss'$ pair.

This independence can be understood in the following ways.
First, note that $\mathcal{K}^{ss'}$ is independently stationary for each $ss'$ pair.
This becomes particularly apparent when considering how one would solve for the coefficients in the uncoupled case where $ss'\to\ell$.
Here, each partial wave is completely independent of one another and thus it would not make sense to mix the VPs or the $\coeff$ across values of $\ell$.
Now consider adiabatically turning on a coupling between two partial waves $s$ and $s'$: the coefficients $\coeffs^{(s)}$ and $\coeffs^{(s')}$ should remain nearly fixed to their previously uncoupled values, but now there is a new set of coefficients to determine, $\coeffs^{(ss')}$.
Thus, even in the coupled case, there are multiple independent sets of coefficients to determine: one for each pair of $ss'$.

An alternative way to understand how the $\coeffs$ enter in the coupled case is to instead start with the Schrodinger equation and enforce (Petrov-)Galerkin orthogonalization as in @sec-kohn-galerkin.
For the diagonal channels, the test functions are chosen to have the same outgoing channel as the trial functions, making the procedure of standard Galerkin form.
But for the off-diagonal channels, the test functions have a different outgoing channel ($s$) than the trial functions ($s'$).
Because the basis of test functions differs from the basis of the trial function, this is instead a Petrov-Galerkin approach.
The linear equations to be solved are exactly what one would obtain from enforcing stationarity in @eq-kvp_reduced_coupled for each $ss'$ independently.

### Generalizations to the continuum states of the higher-body systems {#sec-higher-body-continuum}

The calculations of few-body continuum states are already computationally expensive, starting with three-body systems. Therefore emulators for these calculations are desirable for enabling efficient model calibrations against experimental measurements, e.g., fixing  modern nucleon interactions based on Chiral effective field theory against nucleon-deuteron scattering data. 
 
The KVP has in fact been applied directly to compute three-nuclon continnum states in nuclear physics, including nucleon-deuteron elastic scatterings below and above the deuteron break-up threshold. Of course, there are other direct ways to perform high-fidelity calculations of three-body continuum states in momentum or coordinate space through Faddeev formalism. Lately, the KVP-based two-body emulator has been generalized to three-identifcal-boson systems, particularly one boson elastic scattering off two-boson bound state below the break-up threshold. For elastic scatterings below or above the break-up treshold, the scattering S-matrix can be estimated via a functional that resembles the KVP functional used in two-body case
$$
\begin{aligned}
 \mathcal{S}[\psi] = S - \frac{i}{3\mathcal{N}^2}  \braket{\psi | [H - E] | \psi} 
\end{aligned}
$$ {#eq-three-body-elastic-scattering-KVP-functional}  
Here, we focus on spinlese particles without any relative angular momenta. $S$ is the S-matrix associated with the corresponding trial wave function $|\psi\rangle$ (note this is the full three-body wave function, NOT the so-called Faddeev component), with the wave function's asymptotic behavior 
$$
\begin{aligned}
\langle {R}_1, {r}_1|\psi\rangle 
      \overset{R_1\to \infty}{\longrightarrow} \frac{\mathcal{N}}{\sqrt{v} } \frac{u_B({r}_1) }{ r_1 R_1} 
     \bigl(-e^{-i P R_1} + S\, e^{i P R_1} \bigr) . 
\end{aligned}
$$ {#eq-WFbelowbreakupAsym}
$R_1, r_1$ is one of three different Jacobi coordinates. $v$ is the relative velocity between the scattering particles in the incoming channel, $\mathcal{N}$ the normalization constant that have appeared in @eq-three-body-elastic-scattering-KVP-functional, and $u_B(r)_1$ as the radial wave function of the two-body bound state. 

Following the basic procedure outlined above for two-body emulators, we can perform high-fieldity calculations of $| \psi \rangle$ at various points in the parameter space (e.g., interaction potentials) at the off-line stage, known as snap shots or training calculations. At the on-line stage (emulation), we construct the trial solution to be used in the variational functional as a linear combination of those snapt shots. The same set of the low-dimensional linear equations could be derived for fixing the weights in this linear construction, and the variational functions with these inputs produce accurate results for $S$ matrix at emulation points. This is all straightforward, if we vary only the three-body interaction, while the two-body interaciton is fixed. If the latter is also varied, the two-body bound states at each snap shots could be different among themselves and the linear combination of the snap shots don't have proper asymptotic behavio as in @eq-WFbelowbreakupAsym. As shown in latest work, modifications can be applied to the linear construction to satisfy the condition. The resulted emulator is again a low-dimensional equation system, but the matrix elements lose the affine structure that is a key for fast emulation. Different hyper-reduction approaches could be applied, includig EIM and statistical learning method. In the latest work, we apply the GP to emulate those matrix elements in the parameter space, considering their dependence in the parameters are much smoother, as compared to that of the scattering observable's dependence on those parameters.  

The results for the emulation below the break-up treshold are very encouraging. It should be straight forward to genearlize it to elastic scattering above the break-up threshold. More work needs to be done for generalization to emulating the break-up processes, and even higher-body systems. Of course, proper Fermi statistics, spin and isospin degrees of freeom, and partial waves beyond the s-wave need to be included as well to realize emulation for three-nucleon continuum states.  



### Relating Kohn to Galerkin Orthogonality {#sec-kohn-galerkin}

Rather than starting with a variational principle, the Galerkin approach starts with the Schrodinger equation.
Like the variational approach, it expands $\ket{\psi}$ as a linear combination of known functions (snapshots), but determines the basis coefficients by enforcing orthogonality against a set of *test functions*.
To arrive at the KVP, we choose the test function to be the scattering wave: $\ket{\chi} = \ket{\psi} - \ket{\phi} = G_0 V \ket{\psi}$.
The resulting set of equations is equivalent to those that follow from making the KVP stationary.


The weak form of the Schrodinger equation with our asymptotic constraint is given by
$$
    \braket{\chi | [H - E] | \trial\chi} = - \braket{\chi | V | \phi}
$$
where we have left multiplied the Schrodinger equation by $\bra{\chi}$ and used the Lippmann Schwinger equation $\ket{\trial\psi} = \ket{\phi} + \ket{\trial\chi}$.
By restricting the trial and test functions to finite bases, the set of constraints provided by the Galerkin approach is then
$$
    \braket{\chi_i | [H - E] | \chi_j} \coeffs_j = -\braket{\chi_i | V | \phi}
$$
for all $i = 1, \dots, \nbasis$.
If one is so inclined, this equation can be made symmetric by using integration by parts and noting that the boundary term vanishes if $\ell' = \ell \mod 2$.
The symmetric linear equation then becomes
$$
\begin{aligned}
    A_{ij}\weights_j & = b_i \\
    A_{ij} & =  \braket{\chi_i | [H - E] | \chi_j} + (i\leftrightarrow j) \\
    b_i & = - 2\braket{\chi_i | V | \phi}
\end{aligned}
$$
Although it does not look like it, this is exactly the linear equation to be solved for the KVP emulator (barring the Lagrange multiplier term, which should be included above to enforce the normalization condition).
Starting with the KVP and using the LS equation for wave functions,
$$
\begin{aligned}
    \mathcal{K} & = K - 2\mu \braket{\psi | [H - E] | \psi} \notag\\
    & = K \notag \\
    & \quad - 2\mu \braket{\phi | [H - E] | \phi} 
     - 2\mu\braket{\chi | [H - E] | \chi} \notag \\
    & \quad - 2\mu \braket{\phi | [H - E] | \chi} - 2\mu \braket{\chi | [H - E] | \phi} \notag\\
    & = - 2\mu \braket{\phi | [H - E] | \phi}  - 2\mu\braket{\chi | [H - E] | \chi} \notag\\
    & \quad - 2\mu \braket{\phi | V | \chi} - 2\mu \braket{\chi | V | \phi}
\end{aligned}
$$
where we used (via integration by parts)
$$
    2\mu\left[\braket{\phi | [H - E] | \chi} - \braket{\phi | [H^{\dagger} - E] | \chi}\right] = K.
$$
Inserting the trial and test bases while enforcing stationarity in the above equation yields the linear equation found via the Galerkin method (in its symmetric form).
One of the benefits of the variational approach is that the matrices obtained are always symmetric, which has numerical benefits.
Furthermore, since the KVP itself is a functional for $K$, one can use the KVP to emulate $K$, which might not have been immediately apparent from the straightforward Galerkin approach.

## Newton Emulators {#sec-newton} 

The Lippmann-Schwinger (LS) equation provides an alternative to scattering problems and is equivalent to solving the Schrodinger equation.
The LS equation is an integral equation, rather than a differential equation, whose solution is the reactance matrix $K$ (or, the $T^{\pm}$ matrices). 
This approach is particularly useful when in momentum space, where the $K$ matrix results from a simple matrix solve operation, which sidesteps the need for a differential equation solver.
It is possible to build a reduced-order model directly from the LS equation by using the Newton variational principle.



We aim to construct an efficient emulator for the LS equation given a short-range potential $V(\param)$ that depends smoothly on a set of parameters $\param$, such as the low-energy couplings of a chiral potential.
Specifically, we consider here the LS equation for the scattering $K$ matrix, which reads in operator form^[
All subsequent equations work for any boundary conditions imposed via $G_0$, although we use its principal value formulation here.
That is, using $G_0^{(\pm)}$ and making the replacement $K \to T^{(\pm)}$ will yield an emulator for $T^{(\pm)}$. 
]
$$
\begin{aligned}
    K = V + V G_0 K,
\end{aligned}
$$ {#eq-LS}
with the free-space Green's function operator $G_0(E_q)$ at the on-shell energy $E_q = q^2/2\mu$ and reduced mass $\mu$. 
The energy dependence is implicit in what follows.
We stress that using the $K$ matrix is just a convenient choice. In fact, $T^{(\pm)}$ can be emulated by imposing the associated boundary conditions on $G_0$.
Although the @eq-LS has the formal solution
$$
    K = \left(\identity - V G_0 \right)^{-1} V,
$$ {#eq-LS_formal_sol}
evaluating @eq-LS_formal_sol in a given basis can be prohibitively slow for large-scale Monte Carlo sampling because of the fine (quadrature) grids typically necessary to obtain high-accuracy results. 

Instead of solving @eq-LS directly for each sampling vector $\param$, we propose a variational approach
starting with a trial $K$ matrix as in the RBM approach:
$$
    \trial K(\coeffs) = \sum_{i=1}^{\nbasis}\coeff_i K_i.
$$ {#eq-TritzTrial}
Here, $\{K_i \equiv K(\param_i) \}_{i=1}^{\nbasis}$ are the exact solutions of the @eq-LS for the training set $\{\param_i\}_{i=1}^{\nbasis}$, while $\{\coeff_i\}_{i=1}^{\nbasis}$ are a priori unknown coefficients.^[
The coefficients are not normalized, \ie,  $\sum_{i=1}^{\nbasis}\coeff_{i} \neq 1$, as opposed to the Kohn variational approach in @Furnstahl:2020abp.
]
To determine these coefficients at each $\param$, we apply Newton's variational method [@newton2002scattering; @Rabitz:1973nm], which provides a stationary approximation to the exact scattering $K$ matrix using the functional
$$
\begin{split}
    \mathcal{K}[\trial K] &= V + V G_0 \trial K + \trial K G_0 V \\
    & \quad - \trial K G_0 \trial K + \trial K G_0 V G_0 \trial K,
\end{split}
$$ {#eq-LS_identity}
given a trial matrix $\trial K$ such as the one in @eq-TritzTrial.
The functional @eq-LS_identity is stationary about exact solutions of the LS equation, i.e., $\mathcal{K}[K+\delta K] = K + (\delta K)^2$.

In practice, we determine the stationary solution of the functional @eq-LS_identity in a chosen basis and emulate the scattering $K$~matrix as the matrix element $\braket{\phi' | K | \phi}$. For example, one could choose $\ket{\phi}$ to be a plane-wave partial-wave basis $\ket{k\ell m}$ with momentum $k$ and angular momentum quanta $(l,m)$, or one could keep the angular dependence explicit via $\ket{\phi}=\ket{\mathbf{k}}$ in a single-particle basis.
We are interested in emulating $K$ at the on-shell energy $E_q$, so then $k = k' = q$ for $\ket{\phi}$ and $\bra{\phi'}$.
Expressed in the chosen basis, simplifying the functional @eq-LS_identity after inserting @eq-TritzTrial yields
$$
    \braket{\phi' | \mathcal{K}(\param, \coeffs) | \phi} = \braket{\phi' | V(\param) | \phi} + \coeffs^\trans \vec{m}(\param) - \frac{1}{2} \coeffs^\trans  M(\param) \coeffs,
$$ {#eq-LS_identity_beta}
with
$$
    m_i(\param) = \bra{\phi'} \left[K_i G_0 V(\param) + V(\param) G_0 K_i\right] \ket{\phi},
$$ {#eq-m_vec}
and
$$
\begin{aligned}
    M_{ij}(\param) & = \bra{\phi'} [
                       K_i G_0 K_j - K_i G_0 V(\param) G_0 K_j \notag \\
    & \hspace{0.29in} + K_j G_0 K_i - K_j G_0 V(\param) G_0 K_i
    ] \ket{\phi}.
\end{aligned}
$$ {#eq-M_mat}
If the potential $V(\param)$ is linear in the parameter vector $\param$,
then $\vec{m}$ and $M$
can be efficiently reconstructed by linear combinations of matrices pre-computed during the training phase of the emulator.
This results in substantial improvements in CPU time, *e.g.*, for chiral nucleon-nucleon (NN) interactions.

By imposing the stationary condition $\textup{d} \mathcal{K} /\textup{d} \coeffs = 0$, one then finds
$\coeffsopt(\param)$ such that $M \coeffsopt = \vec{m}$.
Given that the optimal $\coeffsopt(\param)$ yields a trial matrix @eq-TritzTrial with an error $\delta K$, we insert $\coeffsopt$ in @eq-LS_identity_beta to obtain an error $(\delta K)^2$.
The resulting emulator $\mathcal{K}_\star(\param) \equiv \mathcal{K}(\param, \coeffsopt)$ is then
$$
    \braket{\phi' | K  | \phi} \approx
    \braket{\phi' | \mathcal{K} | \phi} =
    \braket{\phi' | V | \phi} + \frac{1}{2} \vec{m}^\trans M^{-1} \vec{m}.
$$ {#eq-emulator}
Equations [-@eq-m_vec]--[-@eq-emulator] are the main expressions for emulating scattering observables
with short-range interactions.
We extend the implementation to the long-range Coulomb potential in Sec.~\ref{sec:coulomb}.^[
Emulating the wave function could follow from working out $\ket{\psi(\param)} = \ket{\phi} + G_0 \mathcal{K}_\star(\param)\ket{\phi}$, where again the appropriate boundary conditions are implied by the choice of $G_0$ [@newton2002scattering; @taylor2006scattering].
]


## Schwinger Emulators {#sec-schwinger}

The Schwinger variational principle (SVP) is given by
$$
\mathcal{K}[\trial\psi] = \braket{\trial\psi | V | \phi} + \braket{\phi | V | \trial\psi} - \braket{\trial\psi | V - V G_0 V | \trial\psi}
$$ {#eq-schwinger-vp}
This too has the stationary property $\mathcal{K}[\psi + \delta\psi] = K + \mathcal{O}(\delta K)^2$ when $\psi$ is a wave function satisfying the LS equation.
The SVP is known to have better numerical properties comparied to the KVP.
For example, @Takatsuka1981SchwingerKohnRelationship showed that one can write down a variational principle of increasing order, of which the KVP is the leading order and the SVP is second order.
Because the SVP is of higher order than the KVP, then given identical trial functions, the SVP will tend to be more accurate [@Takatsuka1981SchwingerKohnRelationship].
However it requires matrix products with a Greens function and is quadratic in $V$, both of which complicate its evaluation.
Furthermore, although many have claimed that it is free of anomalies, the SVP does actually suffer from anomalies as well [@ADHIKARI1991435].
The SVP has been applied successfully in various settings in both nuclear physics and quantum chemistry, including [EXAMPLES].
However, it has not, to our knowledge, been applied in the spirit of the RBM using a snapshot-based approach.

If we follow the MOR philosophy and insert a trial function $\trial\psi$, then the stationary condition becomes
$$
W_{ij}\coeff_j = w_i
$$ {#eq-schwinger-linear}
where
$$
\begin{aligned}
W_{ij} & = \braket{\psi_i | V - V G_0 V | \psi_j} + \braket{\psi_j | V - V G_0 V | \psi_i} \\
w_i & = \braket{\psi_i | V | \phi} + \braket{\phi | V | \psi_i}
\end{aligned}
$$
for all $i = 1, \dots, \nbasis$.

This same system of equations can be determined via a Galerkin projection procedure.
In this case, we start with the LS equation for wave functions
$$
\ket{\psi} = \ket{\phi} + G_0 V \ket{\psi}
$$ {#eq-ls-wave-function}
and create a weak form by left multiplying by the test function $\ket{\zeta}$:
$$
\braket{\zeta | \psi} = \braket{\zeta | \phi} + \braket{\zeta | G_0 V | \psi}
$$
The weak form can then be converted to its discrete form by setting $\psi \to \trial\psi$ and enforcing orthogonality against $\ket{\zeta_i} = V \ket{\psi_i}$ for $i = 1, \dots, \nbasis$.
This yields
$$
Q_{ij} \coeff_j = \braket{\psi_i | V | \phi}
$$ {#eq-schwinger-linear-asymmetric}
where
$$
Q_{ij} = \braket{\psi_i | V - V G_0 V | \psi_j}
$$
The symmetrized version is obtained by creating the reduced weak form starting with $\bra{\psi}$.
Because the reduced form from $\ket{\psi}$ and $\bra{\psi}$ yield identical $\coeffs$, adding them together yields @eq-schwinger-linear.
Thus, the coefficients found by making @eq-schwinger-vp stationary are identical to those found via the Galerkin procedure for @eq-ls-wave-function.

Starting from the Schwinger variational principle has the benefit of yielding an emulator not only for $\psi$, but also for $K$.
If one had not known of the Schwinger variational principle beforehand, it may seem that the Galerkin procedure only provides an emulator for $\psi$.
However, it turns out that one can emulate $K$ in a manner that is equivalent to the Schwinger variational principle.
By inserting the coefficients found via @eq-schwinger-linear into the definition of $\trial\psi$, one can write
$$
\begin{aligned}
\braket{\phi' | K | \phi} = \braket{\phi' | V | \psi} & \approx \braket{\phi' | V | \trial\psi} \notag \\
& = \sum_{ij} \braket{\phi' | V | \psi_i} Q^{-1}_{ij} \braket{\psi_j | V | \phi}.
\end{aligned}
$$
This is exactly the solution for $K$ found via the LS equation while assuming a finite-rank approximation for $V$:
$$
V^{f} = \sum_{ij} V \ket{\psi_i} \Lambda_{ij} \bra{\psi_j} V
$$
where
$$
\Lambda_{ij}^{-1} = \braket{\psi_i | V | \psi_j}.
$$
It is known that the Schwinger variational principle yields a $K$ matrix that is equivalent to that found via a finite-rank approximation to $V$ [CITE], which shows that the Galerkin projection described in this section is identical to the Schwinger variational principle.

## Origin Emulators {#sec-scattering-origin} 


::: {.content-hidden when-format="pdf"}

## Example

```{python}
%load_ext autoreload
%autoreload 2
%matplotlib inline
%config InlineBackend.print_figure_kwargs = {"bbox_inches": None, "facecolor": "w"}

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from emulate import fourier_transform_partial_wave, gaussian_radial_fourier_transform
from emulate.utils import (
    yamaguchi_form_factor_momentum_space,
    yamaguchi_form_factor_position_space,
)
from emulate import CompoundMesh, QuadratureType
from emulate.graphs import PRED_KWARGS, BASIS_KWARGS, FULL_KWARGS
from emulate import setup_rc_params
from emulate import NewtonEmulator
from emulate import SeparableKohnEmulator
from emulate import KohnLippmannSchwingerEmulator
from emulate import BoundaryCondition

setup_rc_params()
sns.set_palette('pastel')
```




```{python}
#| tags: [parameters]

hbar2_over_2mu = 1
nugget = 1e-6
n_train = 5
ell = 0
```


Create a compound mesh object for Gaussian quadrature needed below.
```{python}
n_intervals = 5
nodes = np.linspace(0, 10, n_intervals)
n_points = 40 * np.ones(n_intervals, dtype=int)
mesh = CompoundMesh(nodes, n_points)

# Use the same mesh for k and r
k, dk = mesh.x, mesh.w
r, dr = mesh.x, mesh.w
```



```{python}

# betas = [2, 3]
betas = [2, 4]
# q_cm = np.array([0.5, 1, 2])
# t_cm = np.arange(1, 100, 0.5)
# q_cm = t_cm_to_q_cm(t_cm, MN, MN)
# q_cm = np.linspace(0.1, 2, 101)
q_cm = np.round(np.arange(0.1, 2.01, 0.01), 2)

f_k = np.array(
    [
        yamaguchi_form_factor_momentum_space(k=k, beta=beta, ell=ell, hbar2_over_2mu=hbar2_over_2mu)
        for beta in betas
    ]
)
f_r = np.array(
    [
        yamaguchi_form_factor_position_space(r=r, beta=beta, ell=ell, hbar2_over_2mu=hbar2_over_2mu)
        for beta in betas
    ]
) * r


kohn = SeparableKohnEmulator(
    v_r=f_r,
    r=r,
    dr=dr,
    v_k=f_k,
    k=k,
    dk=dk,
    q_cm=q_cm,
    ell=ell,
    is_local=False,
    nugget=nugget,
    use_lagrange_multiplier=False,
)

rng = np.random.default_rng(13)
params_dimension = len(betas) * (len(betas) + 1) // 2
# p_train = rng.uniform(-50, 50, (n_train, params_dimension))
# p_valid = rng.uniform(-50, 50, params_dimension)

p_train = 1e2*rng.uniform(-5, 5, (n_train, params_dimension))
p_valid = 1e2*rng.uniform(-5, 5, params_dimension)

kohn.fit(p_train)
```

```{python}
n_form_factors = len(f_k)
V_k_yama = []
for i in range(n_form_factors):
    for j in range(i, n_form_factors):
        if i != j:
            V_k_yama.append(f_k[i][:, None] * f_k[j] + f_k[j][:, None] * f_k[i])
        else:
            V_k_yama.append(f_k[i][:, None] * f_k[j])
V_k_yama = np.dstack(V_k_yama)
V_0_yama = np.zeros(V_k_yama.shape[:-1])

newton_yama = NewtonEmulator(
    V0=V_0_yama,
    V1=V_k_yama,
    k=k,
    dk=dk,
    q_cm=q_cm,
    boundary_condition=BoundaryCondition.STANDING,
    nugget=nugget,
)
newton_yama.fit(p_train)
```

```{python}
K_yama_nvp = newton_yama.predict(p_valid)
K_yama_kvp = kohn.predict(p_valid)
K_yama_exact = newton_yama.predict(p_valid, full_space=True)
K_yama_train = kohn.K_train

phase_yama_nvp = newton_yama.phase_shifts(K_yama_nvp)
phase_yama_kvp = newton_yama.phase_shifts(K_yama_kvp)
phase_yama_exact = newton_yama.phase_shifts(K_yama_exact)
phase_yama_train = newton_yama.phase_shifts(K_yama_train)

fig, axes = plt.subplots(2, 1, figsize=(3.4, 3.5), sharex=True)

ax = axes.ravel()[0]
ax.plot(q_cm, phase_yama_train[0], **BASIS_KWARGS, label="Basis")
ax.plot(q_cm, phase_yama_train.T, **BASIS_KWARGS)
ax.plot(q_cm, phase_yama_exact, **FULL_KWARGS, label="Exact")
ax.plot(q_cm, phase_yama_nvp, **PRED_KWARGS, label="NVP")
ax.plot(q_cm, phase_yama_kvp, **PRED_KWARGS, label="KVP")
ax.set_ylabel("$\delta$ [deg]")
ax.set_title("Yamaguchi Potential Phase Shifts")
ax.legend(loc="upper right")
ax.axhline(0, 0, 1, c='k', lw=0.8, zorder=0)
# ax.set_ylim(-5, 5)


ax = axes.ravel()[1]
ax.semilogy(q_cm, np.abs(phase_yama_nvp-phase_yama_exact), label="NVP")
ax.semilogy(q_cm, np.abs(phase_yama_kvp-phase_yama_exact), label="KVP")
ax.legend()
ax.set_ylabel("Absolute Residual [deg]")
ax.set_xlabel(r"$q_{\mathrm{cm}}$ [fm$^{-1}$]")

fig.savefig("figures/phase_shifts_yamaguchi")
plt.show()
```


```{python}
def plot_wave_functions(r, psi_emulator, psi_exact, psi_train, q_cm):
    u_train = psi_train * r
    u_exact = psi_exact * r
    u_valid = psi_emulator * r

    u_resid = np.abs(u_exact - u_valid)

    for i, q_i in enumerate(q_cm):
        max_u_i = max(
            u_train[i].max(),
            u_exact[i].max(),
            u_valid[i].max(),
        )
        u_train[i] = u_train[i] / max_u_i
        u_exact[i] = u_exact[i] / max_u_i
        u_valid[i] = u_valid[i] / max_u_i


    offset = -2.2
    y_ticks = []

    fig, axes = plt.subplots(2, 1, figsize=(3.4, 3.5), sharex=True)
    ax = axes.ravel()[0]

    ax.plot(r, u_train[0, 0, :], **BASIS_KWARGS, label="Basis")
    ax.plot(r, u_exact[0], **FULL_KWARGS, label="Exact")
    ax.plot(r, u_valid[0], **PRED_KWARGS, label="Emulator", c='w')
    for i, q_i in enumerate(q_cm):
        y_i = i * offset
        y_ticks.append(y_i)
        ax.axhline(y_i, 0, 1, c='k', lw=0.8, zorder=-1)
        # ax.plot(r[r_mask], u_train[i, 0].T + i * offset, c=f"C{i}", label=fr"$q_{{ \mathrm{{cm}} }} = {q_i}$", zorder=0)
        ax.plot(r, u_train[i].T + y_i, **BASIS_KWARGS)

        ax.plot(r, u_exact[i] + y_i, **FULL_KWARGS)
        ax.plot(r, u_valid[i] + y_i, **PRED_KWARGS)
        # ax.text(-0.2, y_i-0.3, fr"$q_{{ \mathrm{{cm}} }} = {q_i}$", ha="left", va="top", bbox=dict(boxstyle="round", fc="0.95", ec=f"C{i}"))
        ax.text(-0.65, y_i, fr"$q_{{ \mathrm{{cm}} }} = {q_i}$", ha="right", va="center",
        bbox=dict(boxstyle="round", fc="0.95", ec=f"C{i}", lw=0.5),
        fontsize=6)


    ax.axhline(0, 0, 1, c='k', lw=0.8, zorder=-1)
    ax.legend(loc="upper right")
    # ax.set_xlabel("$r$")
    # ax.set_ylabel("$u(r)$")
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([None for _ in y_ticks])
    # ax.set_title("Radial wave functions for the Yamaguchi potential")

    ax = axes.ravel()[1]
    for i, q_i in enumerate(q_cm):
        ax.semilogy(r, u_resid[i], label=fr"$q_{{ \mathrm{{cm}} }} = {q_i}$", ls=["-", "--", ":"][i])
    ax.set_xlabel("$r$")
    ax.set_ylabel(r"Abs.\ Residual")
    ax.legend()
    # ax.set_xlim(0, 10)
    return fig, axes
```



```{python}
psi_train_yama = kohn.psi_train
psi_exact_yama = kohn.predict_wave_function(p_valid)
psi_valid_yama = kohn.emulate_wave_function(p_valid)

r_mask = r <= 10
q_mask = np.isin(q_cm, [0.5, 1,  2])
fig, axes = plot_wave_functions(
    r=r[r_mask],
    psi_emulator=psi_valid_yama[q_mask][..., r_mask],
    psi_exact=psi_exact_yama[q_mask][..., r_mask],
    psi_train=psi_train_yama[q_mask][..., r_mask],
    q_cm=q_cm[q_mask]
)
axes[0].set_title("Yamaguchi Radial Wave Functions")

fig.savefig("figures/wave_functions_yamaguchi")
plt.show()
```



```{python}
from emulate import hbar_c, MN
from emulate import t_cm_to_q_cm
from emulate.utils import (
    yamaguchi_form_factor_momentum_space,
    yamaguchi_form_factor_position_space,
    yamaguchi_radial_wave_function,
    yamaguchi_scattering_amplitude,
    schrodinger_residual,
    minnesota_potential_coordinate,
    minnesota_potential_momentum_1S0,
)

# n_intervals = 5
# mesh = CompoundMesh(
#     np.linspace(0, 20, n_intervals), 20 * np.ones(n_intervals, dtype=int)
# )
# r, dr = mesh.x, mesh.w
# k, dk = mesh.x, mesh.w

# And parameters for the potentials & solvers
kappa_r = 1.487
kappa_s = 0.465
kappa_t = 0.639

v_0r = 200.0
v_0s = -91.85
v_0t = -178.0

# t_cm = np.array([50])
# t_cm = np.arange(1, 100, 0.5)
# q_cm = t_cm_to_q_cm(t_cm, MN, MN)
hbar2_over_2mu = hbar_c**2 / MN

# Given potentials from the above parameters that are linear combinations of two matrices
V_k = hbar2_over_2mu ** (-1) * np.stack(
    [
        minnesota_potential_momentum_1S0(k[:, None], k, kappa_r),
        minnesota_potential_momentum_1S0(k[:, None], k, kappa_s),
    ],
    axis=-1,
)
V_r = hbar2_over_2mu ** (-1) * np.stack(
    [
        np.diag(minnesota_potential_coordinate(r, kappa_r)),
        np.diag(minnesota_potential_coordinate(r, kappa_s)),
    ],
    axis=-1,
)

# And the potentials are fed into the emulator classes (with no constant term)
newton_1S0 = NewtonEmulator(
    V0=np.zeros_like(V_k[..., 0]),
    V1=V_k,
    k=k,
    dk=dk,
    q_cm=q_cm,
    boundary_condition=BoundaryCondition.STANDING,
    nugget=1e-10,
)

ls_kohn_1S0 = KohnLippmannSchwingerEmulator(
    V0=np.zeros_like(V_r[..., 0]),
    V1=V_r,
    r=r,
    dr=dr,
    NVP=newton_1S0,
    is_local=True,
    ell=ell,
    # nugget=1e-10,
)

# rng = np.random.default_rng(12)
# p_train = rng.uniform(-200, 200, (n_train, 2))
p_train = np.array([
    [0.0, -291.85],
    [100.0, 8.15],
    [300.0, -191.85],
    [300.0, 8.15],
])

# p_valid = rng.uniform(-, 5, params_dimension)

# When the wave function is predicted at the best fit Minnesota potential values
p_valid = np.array([v_0r, v_0s])

newton_1S0.fit(p_train)
ls_kohn_1S0.fit(p_train)
```


```{python}
psi_train_minn_1S0 = ls_kohn_1S0.psi_train
psi_exact_minn_1S0 = ls_kohn_1S0.predict_wave_function(p_valid)
psi_valid_minn_1S0 = ls_kohn_1S0.emulate_wave_function(p_valid)

r_mask = r <= 10
q_mask = np.isin(q_cm, [0.8, 1,  2])
fig, axes = plot_wave_functions(
    r=r[r_mask],
    psi_emulator=psi_valid_minn_1S0[q_mask][..., r_mask],
    psi_exact=psi_exact_minn_1S0[q_mask][..., r_mask],
    psi_train=psi_train_minn_1S0[q_mask][..., r_mask],
    q_cm=q_cm[q_mask]
)
axes[0].set_title("Radial wave functions for the Minnesota potential")
plt.show()
```

```{python}
ls_kohn_1S0.psi_train.shape
```


```{python}
q_cm
```



```{python}
K_1S0_kvp = ls_kohn_1S0.predict(p_valid, full_space=False)
K_1S0_nvp = newton_1S0.predict(p_valid)
K_1S0_exact = newton_1S0.predict(p_valid, full_space=True)

delta_1S0_kvp = newton_1S0.phase_shifts(K_1S0_kvp)
delta_1S0_train = newton_1S0.phase_shifts(newton_1S0.K_on_shell_train.T).T
delta_1S0_nvp = newton_1S0.phase_shifts(K_1S0_nvp)
delta_1S0_exact = newton_1S0.phase_shifts(K_1S0_exact)

fig, ax = plt.subplots(figsize=(3.4, 3))
# ax.plot(t_cm, newton_1S0.K_on_shell_train[0, :], **BASIS_KWARGS, label="Basis")
# ax.plot(t_cm, newton_1S0.K_on_shell_train, **BASIS_KWARGS)
# ax.plot(t_cm, K_1S0_nvp, **PRED_KWARGS, label="Emulator")
# ax.plot(t_cm, K_1S0_exact, **FULL_KWARGS, label="Exact")
ax.plot(t_cm, delta_1S0_train[:, 0], **BASIS_KWARGS, label="Basis")
ax.plot(t_cm, delta_1S0_train, **BASIS_KWARGS)
ax.plot(t_cm, delta_1S0_exact, **FULL_KWARGS, label="Exact")
ax.plot(t_cm, delta_1S0_nvp, **PRED_KWARGS, label="Emulator")
ax.plot(t_cm, delta_1S0_kvp, **PRED_KWARGS, label="Emulator")

ax.legend()
ax.set_title("1S0 Phase Shifts from the Minnesota Potential")
ax.set_xlabel("$t_{\mathrm{cm}}$ [MeV]")
ax.set_ylabel("$\delta$")
# ax.axis("off")
# ax.set_xticks([])
# ax.set_yticks([])
# ax.margins(0)
# ax.set_ylim(-5, 5)
plt.show()
```

```{python}
newton_1S0.K_on_shell_train.shape
```

```{python}
K_1S0_kvp.shape
```



```{python}
r_mask = r <= 10

fig, ax = plt.subplots(figsize=(3.4, 3))
ax.plot(r[r_mask], u_exact[0, r_mask], **FULL_KWARGS, label="Exact")
ax.plot(r[r_mask], u_valid[0, r_mask], **PRED_KWARGS, label="Emulator", c='w')
for i, q_i in enumerate(q_cm):
    ax.plot(r[r_mask], u_train[i, 0, r_mask].T, c=f"C{i}", label=fr"$q_{{ \mathrm{{cm}} }} = {q_i:0.1f}$", zorder=0, **BASIS_KWARGS)
    ax.plot(r[r_mask], u_train[i][:, r_mask].T, c=f"C{i}", zorder=0)

    ax.plot(r[r_mask], u_exact[i, r_mask], **FULL_KWARGS)
    ax.plot(r[r_mask], u_valid[i, r_mask], **PRED_KWARGS)


ax.axhline(0, 0, 1, c='k', lw=0.8, zorder=-1)
ax.legend()
ax.set_xlabel("$r$")
ax.set_ylabel("$u(r)$")
ax.set_title("Radial wave functions for the Minnesota potential")
plt.show()
```





```{python}
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from emulate import fourier_transform_partial_wave, gaussian_radial_fourier_transform
from emulate.utils import (
    yamaguchi_form_factor_momentum_space,
    yamaguchi_form_factor_position_space,
)
from emulate import CompoundMesh, QuadratureType
from emulate.graphs import PRED_KWARGS, BASIS_KWARGS, FULL_KWARGS
from emulate import setup_rc_params
from emulate import NewtonEmulator
from emulate import SeparableKohnEmulator
from emulate import KohnLippmannSchwingerEmulator
from emulate import BoundaryCondition

setup_rc_params()
sns.set_palette('pastel')

from scipy.integrate import odeint


def compute_U_vector(r, kappas):
    return np.stack([np.exp(-kappa * r**2) for kappa in kappas], axis=-1)

def compute_U(r, params, kappas):
    return np.sum([p * np.exp(-kappa * r**2) for kappa, p in zip(kappas, params)], axis=0)

def solve_schrodinger(y, r, params, kappas, k):
    psi, d_psi = y
    U = compute_U(r, params, kappas)
    d2_psi = U * psi - k**2 * psi
    return [d_psi, d2_psi]


n_intervals = 10
nodes = np.linspace(0, 10, n_intervals)
n_points = 50 * np.ones(n_intervals, dtype=int)
mesh = CompoundMesh(nodes, n_points)

# Use the same mesh for k and r
k, dk = mesh.x, mesh.w
r, dr = mesh.x, mesh.w

r_grid = np.linspace(0, 10, 1001)
q_cm = 1

KAPPAS = [0.5, 1]
rng = np.random.default_rng(1)
p_train = rng.uniform(-5, 5, (6, len(KAPPAS)))

psi_train = []
d_psi_train = []
d2_psi_train = []
initial_values = np.array([0, 1])
for p_i in p_train:
    psi_i, d_psi_i = odeint(solve_schrodinger, y0=initial_values, t=r, args=(p_i, KAPPAS, q_cm), printmessg=True, atol=1e-13, rtol=1e-11).T
    psi_train.append(psi_i)
    d_psi_train.append(d_psi_i)

    d2_psi_i = solve_schrodinger([psi_i, d_psi_i], r, p_i, KAPPAS, q_cm)[-1]
    d2_psi_train.append(d2_psi_i)
psi_train = np.array(psi_train)
d_psi_train = np.array(d_psi_train)
d2_psi_train = np.array(d2_psi_train)

fig, ax = plt.subplots(figsize=(3.4, 3))
ax.plot(r, psi_train.T, c='k')
ax.plot(r, d_psi_train.T, ls="--", c='b')
ax.plot(r, d2_psi_train.T, ls="-.", c='lightgrey')

ax.axhline(0, 1, 0, c='k', lw=0.8, zorder=-1)
ax.set_xlim(0, 10)
ax.set_ylim(-5, 5)


U1 = compute_U_vector(r, KAPPAS)
nabla_proj = np.einsum("ir,jr->ij", dr * psi_train, d2_psi_train)
U_proj = np.einsum("ir,rp,jr->ijp", dr * psi_train, U1, psi_train)
norm_proj = np.einsum("ir,jr->ij", dr * psi_train, psi_train)


```



```{python}
bc_idx = 0
# Use a Petrov-Galerkin approach for the test functions on the boundary. Use d_psi.
BC_norm_proj = d_psi_train[:, bc_idx] * d_psi_train[:, bc_idx].T
b = d_psi_train[:, bc_idx]

p_valid = np.random.default_rng(29).uniform(-5, 5, len(KAPPAS))

R0 = - nabla_proj - q_cm**2 * norm_proj + BC_norm_proj
# R0 += 1e-15 * np.eye(R0.shape[0])
R1 = U_proj

psi_exact, _ = odeint(solve_schrodinger, y0=initial_values, t=r, args=(p_valid, KAPPAS, q_cm)).T

coeff = np.linalg.solve(R0 + R1 @ p_valid, b)
psi_valid = coeff @ psi_train

fig, axes = plt.subplots(2, 1, figsize=(3.4, 4), sharex=True)

r_mask = r <= 10

ax = axes.ravel()[0]
ax.plot(r[r_mask], psi_train[0, r_mask].T, **BASIS_KWARGS, label="Train")
ax.plot(r[r_mask], psi_train[:, r_mask].T, **BASIS_KWARGS)
# ax.plot(r, d_psi_train.T, ls="--", c='b')
# ax.plot(r, d2_psi_train.T, ls="-.", c='lightgrey')
ax.plot(r[r_mask], psi_exact[r_mask], **FULL_KWARGS, label="Exact")
ax.plot(r[r_mask], psi_valid[r_mask], **PRED_KWARGS, label="Emulator")
ax.set_xlim(0, 10)
# ax.set_xlabel("$r$")
ax.set_ylabel("$u(r)$")
ax.legend()
ax.set_title("Galerkin approach for $u(r)$ with Minnesota potential")


ax = axes.ravel()[1]

ax.semilogy(r[r_mask], np.abs(psi_exact[r_mask]-psi_valid[r_mask]), label="Abs. Residual")
ax.set_xlabel("$r$")
ax.legend()
plt.show()
```

```{python}
r[bc_idx]
```


```{python}
print(coeff, coeff.sum())
```

:::
