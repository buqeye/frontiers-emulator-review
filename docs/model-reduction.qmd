---
output: html_notebook
format:
  html:
    code-fold: show
    code-tools: true
jupyter: python3
citation: true
appendix-style: default
license: "CC BY"
---


# Model Reduction {#sec-model-reduction}


::: {.hidden}
{{< include macros.qmd >}}
:::


## What is model reduction?

The holy grail of complex physical simulations is an *emulator* that captures most important features of the simulator, but is much cheaper to evaluate.
The field of model reduction provides a set of tools for constructing such emulators, or reduced-order models, in a variety of contexts.
For instance, [@Trotta:2008qt]


## Gaussian Processes

The complexity of a simulator can be alleviated by training an emulator on evaluations of the simulator at a few well chosen points.
Subsequent evaluations are cheap because the emulator can be used in place of the simulator.
Gaussian processes are one such flexible emulator.

A Gaussian process (GP) is a non-parametric model used for both regression and classification tasks.
Like its multivariate counterpart, GPs are fully specified by their mean and covariance.
But draws from a GP are functions $f(x)$, which makes the mean $m(x)$ and covariance $\Sigma(x, x')$ also functions of $x$.
However, one can relate these back to less abstract multivariate distribution via the defining property of a GP:

> A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.

GPs share the features that make Gaussian distributions attractive in other contexts:

* simple, closed-form expressions result from Gaussian distributions
* Gaussians closely approximate many naturally occuring phenomena
* ...

This has led to its incorporation into many machine learning libraries, such as `scikit-learn` and `Tensorflow`, and has found widespread use across disciplines ranging from geography to biology.
Indeed, it is now simpler than ever to emulate high-fidelity models with a GP.

::: {.content-hidden when-format="pdf"}
```{python}
from sklearn.gaussian_process import GaussianProcessRegressor
```
:::


## Variational principles & the Ritz method

Variational principles are ubiquitous in physics.
Many differential equations have a corresponding action, where the solution to the differential equation also makes the action stationary.
This yields an alternate way of solving a set of PDEs: rather than solving the Euler-Lagrange equations themselves, one can instead find the solution that makes the action stationary under perturbations.
The use of variational principles as a means to solve otherwise difficult problems dates back to Ritz.

Variational principles begin with the definition of a scalar functional $\action$ that can be written as
$$
    \action[\psi] = \int_\Omega \dd{\Omega} F[\psi, \psi', \cdots] + \int_\Gamma \dd{\Gamma} E[\psi, \psi', \cdots]
$$
where $F$ and $E$ are known differential operators, and $\Omega$ and $\Gamma$ are the domain and boundary, respectively.
The unknown function $\psi$ is determined as that which makes $\action$ *stationary*, i.e., $\delta\action = 0$.

The use of an *action* $\action$ to derive Euler-Lagrange equations is common practice in the physical sciences.
But if one first proposes an *ansatz* as a trial function
$$
\begin{aligned}
    \trial\psi = \sum_{i=1}^{\nbasis} \coeff_i\psi_i = X\coeffs
\end{aligned}
$$ {#eq-trial_general}
then variational principles lead straightforwardly to an emulator $\trial\psi \approx \psi$.
Rather than stipulate that $\delta\action = 0$ for any arbitrary variation $\delta\psi$, we instead extract the optimal coefficients, $\coeffs_\star$, as those for which $\action$ is stationary under variations in $\coeffs$:
$$
\begin{aligned}
    \delta\action = \sum_{i=1}^{\nbasis} \frac{\partial\action}{\partial\coeff_i}\delta\coeff_i = 0
\end{aligned}
$$ {#eq-action_stationary_ansatz}

The general case would involve a numerical search for the solution to @eq-action_stationary_ansatz or \eqref{eq-action_stationary_ansatz} but if $\action$ is quadratic in $\psi$ then the solution can be determined exactly.
In this case $\action$ can be written as
$$
\begin{aligned}
    \action = \coeffs^\dagger A \coeffs + \vec{b}\cdot \coeffs + c
\end{aligned}
$$
for some $A$, $\vec{b}$, and $c$.
The quadratic portion could be made symmetric---if it is not already---by writing it as
$$
\begin{aligned}
    \action & = \coeffs^\dagger A_s \coeffs+ \vec{b}\cdot \coeffs + c \\
    A_s & = \frac{A + A^\dagger}{2}
\end{aligned}
$$
which can be desirable for numerical purposes.
It then follows that the optimal coefficients, $\coeffs_\star$ are those for which
$$
\begin{aligned}
    \delta\action = A_s\coeffs_\star + \vec{b} = 0
\end{aligned}
$$ {#eq-coefficient_solve_quadratic}
which requires a simple matrix solve operation to obtain $\coeffs_\star$.
This solve occurs only in a space of size $\nbasis$, the number of basis elements $\{\psi_i\}$, rather than in the much larger space of $\psi$ itself.
Therefore, so long as $\{\psi_i\}$ approximately span the space in which $\psi$ lives, the trial function constructed by @eq-trial_general and @eq-coefficient_solve_quadratic will be both a fast and an accurate emulator of $\psi$.


## Snapshots
