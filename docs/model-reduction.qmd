---
output: html_notebook
format:
  html:
    code-fold: show
    code-tools: true
jupyter: python3
citation: true
appendix-style: default
license: "CC BY"
---

::: {.hidden}
$$
{{< include mymacros.tex >}}
$$
:::


# Model Reduction {#sec-model-reduction}
$\identity$
In this Section, we provide a more general discussion of variational principles and the Galerkin method as the foundations for constructing highly efficient emulators for nuclear physics.
The general methods discussed here will be used as a springboard to develop emulators for the specific case of scattering systems in Section @sec-scattering-emulators.

We consider (time-independent) differential equations that depend on the parameter vector $\params$ and
aim to find the solution $\trialfunc$ of
$$
\begin{align}
    D(\trialfunc; \params) & = 0 \quad \text{in } \Omega,  \\
    B(\trialfunc; \params) & = 0 \quad \text{on } \Gamma,
\end{align}
$$ {#eq-generic_differential_and_boundary}
where $\{D, B\}$ are differential operators and $\Omega$ is the domain with boundary $\Gamma$.
Here, we use the generic function $\trialfunc$ because different choices of $\trialfunc$ will be made in Section @sec-scattering-emulators.
In what follows, we will discuss two related methods for constructing emulators from Equation @eq-generic_differential_and_boundary,
which states the physics problem in a _strong form_ (_i.e._, for each point in the domain and on the boundary).
The first begins by finding a variational principle whose stationary solution implies Equation @eq-generic_differential_and_boundary.
The second instead constructs the corresponding _weak form_ of Equation @eq-generic_differential_and_boundary.


## Variational principles {#sec-variational}

Variational principles (VPs) have a long history in physics and play a central role in a wide range of applications; _e.g._, for deriving equations of motion using stationary-action principles and Euler--Lagrange equations in classical mechanics (see, _e.g._, Ref. [@Variational_Ritz_history_review_2012] for a historical overview).
Here, we use VPs as an alternate way of solving  the differential equations @eq-generic_differential_and_boundary.

Variational principles are based on scalar functionals of the form
$$
\begin{equation}
    \action[\trialfunc] = \int_\Omega \dd{\Omega} F[\trialfunc] + \int_\Gamma \dd{\Gamma} G[\trialfunc],
\end{equation}
$$ {#eq-generic_functional}
where $F$ and $G$ are differential operators.
Many differential equations @eq-generic_differential_and_boundary can be obtained as stationary solutions of a corresponding functional @eq-generic_functional; _i.e._, the solution $\trialfunc_\star$ that leads to $\delta\action[\trialfunc_\star] = 0$.

However, VPs can also lead straightforwardly to a reduced-order model.
This follows from the following trial ansatz
$$
\begin{align}
    \ket{\trial\trialfunc} & \equiv \sum_{i=1}^{\nbasis} \coeff_i\ket{\trialfunc_i} = X\coeffs, %\label{eq-trial_general_xi}
    \\
    X & \equiv
    \begin{bmatrix}
        \ket{\trialfunc_1} & \ket{\trialfunc_2} & \cdots & \ket{\trialfunc_{\nbasis}}
    \end{bmatrix},
\end{align}
$$ {#eq-trial_general_subeq}
with the to-be-determined coefficients vector $\coeffs$.
Rather than stipulate that $\delta\action = 0$ for any arbitrary variation $\delta\trialfunc$, we instead extract the optimal coefficients, $\coeffsopt$, as those for which $\action$ is stationary under variations in $\coeffs$:^[For simplicity we consider $\trialfunc$ to be real a variable; for complex variables, independent variations $\delta\coeffs^\ast$ should be included in the discussion.]
$$
\begin{equation} %\label{eq-action_stationary_ansatz}
    \delta\action = \sum_{i=1}^{\nbasis} \frac{\partial\action}{\partial\coeff_i}\delta\coeff_i = 0.
\end{equation}
$$ {#eq-action_stationary_ansatz}
The general case would involve a numerical search for the solution to Equation @eq-action_stationary_ansatz but if $\action$ is quadratic in $\trialfunc$ then the solution can be determined exactly.
In this case, $\action$ can be written as
$$
\begin{equation}
    \action = \frac{1}{2}\coeffs^\trans A \coeffs + \vec{b}\cdot \coeffs + c
\end{equation}
$$
for some matrix $A$, vector $\vec{b}$, and scalar $c$.
Symmetrizing the quadratic portion---if it is not already symmetric---by rewriting $A \leftarrow (A + A^\trans)/2$ can be desirable for numerical purposes.
It then follows that the optimal coefficients, $\coeffsopt$ are those for which
$$
\begin{equation} %\label{eq-coefficient_solve_quadratic}
    \delta\action = A_s\coeffs_\star + \vec{b} = 0,
\end{equation}
$$ {#eq-coefficient_solve_quadratic}
which can be solved for $\coeffsopt$ using standard linear algebra methods.
Solving for $\coeffsopt$ occurs only in a space of size $\nbasis$, the number of basis elements $\{\trialfunc_i\}_{i=1}^{\nbasis}$, rather than in the much larger space of $\trialfunc$ itself.
Therefore, as long as $\{\trialfunc_i\}_{i=1}^{\nbasis}$ approximately spans the space in which $\trialfunc$ lives, the trial function constructed by Equations @eq-trial_general_subeq and @eq-coefficient_solve_quadratic will be both a fast & accurate emulator of $\trialfunc$.


## Galerkin Emulators

On the other hand, the Galerkin approach, also more broadly called the "method of weighted residuals," relies on the _weak} formulation of the differential equations in Equation @eq-generic_differential_and_boundary.
To obtain the weak form, the differential equation and boundary condition (in Equation @eq-generic_differential_and_boundary}) are left-multiplied by arbitrary test functions $\testfunc$ and $\bar\testfunc$ and integrated over the domain and boundary, respectively, and then their sum is set to zero:
$$
\begin{equation} %\label{eq-weak_differential}
    \int_\Omega \dd{\Omega} \testfunc  D(\trialfunc) + \int_\Gamma \dd{\Gamma} \bar\testfunc  B(\trialfunc) = 0.
\end{equation}
$$ {#eq-weak_differential}
If Equation @eq-weak_differential holds for all $\testfunc$ and $\bar\testfunc$, then Equation @eq-generic_differential_and_boundary must be satisfied as well.
The form of Equation @eq-weak_differential is often rewritten using integration by parts to reduce the order of derivatives and simplify the solution.
Importantly, the weak form has the integral form needed for our emulator application.
The weak form and its Galerkin projection are used extensively, _e.g._, in the finite element method; see References [@zienkiewicz2013finite;@Zienkiewicz2014finitesolid;@Zienkiewicz2014finitefluid] for an in-depth study and examples.
For a discussion of the convergence properties of the Galerkin method, its relation to abstract variational problems, and other salient mathematical details, see References [@hesthaven2015certified; @Mikhlin_1967;@Evans1996;@Brenner:2008].
Here, we follow the introduction of Galerkin methods as provided in Reference @zienkiewicz2013finite.

Starting with the weak form, we can construct an emulator that avoids the need for an explicit variational principle.
It begins by first noting that substituting our trial function Equation @eq-trial_general_subeq into $D(\trialfunc)$ and $B(\trialfunc)$ will not, in general, satisfy Equation @eq-generic_differential_and_boundary regardless of the choice of $\coeffs$.
Therefore, there will be some _residual_, and the goal is to find the $\coeffsopt$ which minimizes that residual across a range of test functions $\testfunc$ and $\bar\testfunc$.
This system would be over-determined in the case of truly arbitrary test functions, so instead, we propose the test bases
$$
\begin{align}
    \ket{\testfunc} & = \sum_{i=1}^{\nbasis} \delta\coeff_i\ket{\testfunc_i},
    \qquad
    \ket{\bar\testfunc} = \sum_{i=1}^{\nbasis} \delta\coeff_i\ket{\bar\testfunc_i},
\end{align}
$$
where $\delta\coeff_i$ are arbitrary parameters, not related to the $\coeff_i$ in Equation @eq-trial_general_subeq.
The $\delta\coeff_i$ will play the same role as those in Equation @eq-action_stationary_ansatz, namely as a bookkeeping method for determining the set of equations that are equivalently zero.
By enforcing that the residuals against these test functions vanish for arbitrary $\delta\coeff_i$, the bracketed expression in
$$
\begin{align}
%    \delta\coeff_i \Bigl[\sum_j \braket{\psi_i | D |\psi_j} \beta_j  +  \sum_j \braket{\bar\phi_i | B |\psi_j} \beta_j
 %   \Bigl] = 0,  \\
    \delta\coeff_i \Bigl[\int_{\Omega} \dd{\Omega}  \testfunc_i  D(X\coeffsopt) +  \int_{\Gamma} \dd{\Gamma} \bar\testfunc_i  B(X\coeffsopt)
    \Bigl]= 0, %\label{eq-weak_form_subspace}
\end{align}
$$ {#eq-weak_form_subspace}
is zero for all $i \in [1, \nbasis]$, from which the optimal $\coeffsopt$ are extracted.
Because this approximately satisfies the weak formulation, we have found an approximate solution to Equation @eq-generic_differential_and_boundary.

In a variety of cases \cite{zienkiewicz2013finite}, the subspace $\mathcal{Z}$ spanned by the test function basis is chosen to coincide with the subspace $\mathcal{X}$ spanned by the trial function basis $X$; _i.e._, $\mathcal{Z} = \mathcal{X}$.
This particular choice is known as _the_ Galerkin method, but it is sometimes further specified as the Ritz--Galerkin or Bubnov--Galerkin methods.
The Galerkin method is more general than the variational methods described in Section @sec-variational because the test space need not be equivalent to the trial space (_i.e._, $\mathcal{Z} \neq \mathcal{X}$).
In these cases, the approach is described as the Petrov--Galerkin method @zienkiewicz2013finite;
this can result in more efficient emulators for some differential equations @Zienkiewicz2014finitefluid.
